{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd2219e8-84e9-47fe-bd5f-b566a36bdba5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Saraga Audiovisual: a large multimodal open data collection for the analysis of carnatic music\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f2edec-a955-405f-8205-5d07621755d9",
   "metadata": {},
   "source": [
    "## 1.Accessing the Saraga Audiovisual Dataset  \n",
    "\n",
    ">Saraga Audiovisual is a dataset that includes diverse renditions of **Carnatic vocal performances**, totaling **42 concerts** and more than **60 hours of music** featuring **Video recordings** for all concerts, enabling a wide range of multimodal analyses and **High-quality human pose estimation data** of musicians."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59d8209-0a29-40c1-ad9c-367b3792f829",
   "metadata": {},
   "source": [
    "### 1.1.Download the Dataset\n",
    "\n",
    "The Saraga dataset is available on Zenodo. You can download it using the following link:  \n",
    "[Zondo Link](https://zenodo.org/records/15102483)  \n",
    "\n",
    "The dataset is split into multiple parts, each containing specific components:\n",
    "\n",
    "- **`saraga_audio.zip`** – Multi-track audio files along with their corresponding mixture files.\n",
    "- **`saraga_gesture.zip`** – Pose estimation files extracted from videos corresponding to each audio track.\n",
    "- **`saraga_metadata.zip`** – Metadata for all the audio files.\n",
    "- **`saraga_video.zip`** – Videos from three sample concerts. Due to size constraints, only these three concerts are included. For access to the full video collection, contact the dataset providers.\n",
    "\n",
    "Visit [Zenodo](https://zenodo.org/records/15102483) and manually download the required zip files.\n",
    "\n",
    "Alternatively, you can use `wget` to download the files directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf331d3e-18ff-4721-a0e2-1b5a57ee4aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O \"saraga_audio.zip\" \"https://zenodo.org/records/15102483/files/saraga%20audio.zip?download=1\"\n",
    "!wget -O \"saraga_gesture.zip\" \"https://zenodo.org/records/15102483/files/saraga%20gesture.zip?download=1\"\n",
    "!wget -O \"saraga_metadata.zip\" \"https://zenodo.org/records/15102483/files/saraga%20metadata.zip?download=1\"\n",
    "!wget -O \"saraga_video.zip\" \"https://zenodo.org/records/15102483/files/saraga%20visual.zip?download=1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55001436-03dd-46c4-a810-2f773e37fe2c",
   "metadata": {},
   "source": [
    "### 1.2. Extract the Dataset\n",
    "\n",
    "Once the files are downloaded, extract them into a common folder.  \n",
    "For that, we use `zipfile`, a Python library for handling zip files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4515ec5-2486-4625-b5e6-3de5e1dc35bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8870eb9-34cc-4168-b4f8-5208fb2f23be",
   "metadata": {},
   "outputs": [],
   "source": [
    "saraga_folder = \"./saraga\"\n",
    "zip_files = [\"saraga gesture.zip\", \"saraga metadata.zip\", \"saraga visual.zip\"]#, \"saraga_audio.zip\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70d8094-8a4b-48fe-ac00-d13e4f0637aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract each zip file to extract path\n",
    "for zip_file in zip_files:\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(saraga_folder)\n",
    "    print(f\"Extracted {zip_file} to {extract_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa84642f-0283-4b64-aab4-cd66014398a2",
   "metadata": {},
   "source": [
    "## 2.Processing Audiovisual Data\n",
    "\n",
    "Now we will process the keypoints from the gesture dataset and display the skeleton on the performance video. For this tutorial, we will use *Valappu Thala* by *Brinda Manickavasagan*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9ee964-57de-484d-b5f7-6cc47da46ff4",
   "metadata": {},
   "source": [
    "### 2.1. Load Gestures and Video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89d4bb3-8487-4e0b-b8af-4ef8803e3318",
   "metadata": {},
   "source": [
    "Let's import the necessary libraries required for processing the video and gestures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29be725-e09e-4d7a-a929-980170184cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18b9b0b9-4b64-4568-9ef8-b912476c16ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "149d5306-a1b4-4cd4-be50-29dfe9582bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define File Paths for the Relevant Performance\n",
    "keypoints_path = \"saraga/saraga gesture/Aditi Prahalad/Ananda Natana Prakasham/singer/singer_0_753_kpts.npy\"\n",
    "scores_path = \"saraga/saraga gesture/Aditi Prahalad/Ananda Natana Prakasham/singer/singer_0_753_scores.npy\"\n",
    "video_path = \"saraga/saraga visual/Aditi Prahlad/Ananda Natana Prakasham/Ananda Natana Prakasham.mov\"\n",
    "save_path = \"output.mp4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afbe44e-f4ca-430e-b9fa-8c7815978617",
   "metadata": {},
   "source": [
    "Load the keypoints and scores file for the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "236754c9-ca5a-4988-87ca-4caacfecd58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints = np.load(keypoints_path)\n",
    "scores = np.load(scores_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4e8812-3bcb-4818-a819-8ada6e5bf0fb",
   "metadata": {},
   "source": [
    "Now, we will define the skeleton—a list of tuples that defines how keypoints should be connected to form a human pose. For example, the left shoulder should be connected to the left elbow, and the elbow to the wrist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4a08cc5-aaa7-405e-8a2f-e181ce33e162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skeleton for 135 keypoints (MMPose)\n",
    "skeleton = [\n",
    "    (0, 1), (1, 2),     # Eyes (left to right)\n",
    "    (0, 3), (0, 4),     # Nose to ears (left and right)\n",
    "    (5, 6),             # Shoulders (left and right)\n",
    "    (5, 7), (7, 9),     # Left arm (shoulder -> elbow -> wrist)\n",
    "    (6, 8), (8, 10),\n",
    "    (11,12),            # Right arm (shoulder -> elbow -> wrist)\n",
    "    (5, 11), (6, 12),   # Shoulders to hips\n",
    "    (11, 13), (13, 15), # Left leg (hip -> knee -> ankle)\n",
    "    (12, 14), (14, 16)  # Right leg (hip -> knee -> ankle)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8cf1a8-ec4d-4e7b-9de9-e987c50c1fa0",
   "metadata": {},
   "source": [
    "Now, we will open the video file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3608cfb6-434d-4507-a87a-39d356b0f359",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(video_path)\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))  # Frames per second\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767d6628-cbfc-4742-8b7f-94416e3d2355",
   "metadata": {},
   "source": [
    "### 2.2.Process the frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e612c425-33b1-41f1-9ef9-12923abe768f",
   "metadata": {},
   "source": [
    "Create a temporary output video file so that we can save the processed video with the overlayed skeleton and gestures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2abe32b2-2e77-4942-83b6-965a196eea97",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076314a4-0a46-4ba5-8fd7-a1c0d1677b00",
   "metadata": {},
   "source": [
    "Now, let's project the skeleton onto the video frames. First, we will select a 20-second segment to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e333230-e811-4010-9e75-c6c806ae0c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = 10  # Start time in seconds (adjust as needed)\n",
    "end_time = start_time + 20  # End time in seconds\n",
    "start_frame = int(start_time * fps)\n",
    "end_frame = int(end_time * fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49bef851-cdea-490b-9c1d-5bffc3906b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_idx = 0\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "    if start_frame <= frame_idx < end_frame:\n",
    "        # Get keypoints and scores for the current frame\n",
    "        if frame_idx < len(keypoints):\n",
    "            frame_keypoints = keypoints[frame_idx]\n",
    "            frame_scores = scores[frame_idx]\n",
    "            \n",
    "            # Draw keypoints and skeleton\n",
    "            for i, (x, y) in enumerate(frame_keypoints):\n",
    "                # Only draw if confidence score is above threshold\n",
    "                if frame_scores[i] > 0.5:  # Adjust threshold as needed\n",
    "                    cv2.circle(frame, (int(x), int(y)), 5, (0, 255, 0), -1)\n",
    "                    \n",
    "            # Draw skeleton\n",
    "            for connection in skeleton:\n",
    "                start, end = connection\n",
    "                if frame_scores[start] > 0.5 and frame_scores[end] > 0.5:\n",
    "                    x1, y1 = frame_keypoints[start]\n",
    "                    x2, y2 = frame_keypoints[end]\n",
    "                    cv2.line(frame, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
    "                    \n",
    "        # Write frame to output video\n",
    "        out.write(frame)\n",
    "        \n",
    "    frame_idx += 1\n",
    "    \n",
    "    # Stop processing after the end frame\n",
    "    if frame_idx >= end_frame:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1452cf-fd2a-4e38-b26a-f38657e72fef",
   "metadata": {},
   "source": [
    "Free the resources after processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19036716-4669-4aa2-a1cd-0cba27f9eaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d568e7f8-3cf8-4194-97e0-085fdaad9bbf",
   "metadata": {},
   "source": [
    "## 3. Display the Result\n",
    "\n",
    "Great! We have the results. Now, we can display the video in the notebook using `IPython.display`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a92b60c0-901a-4b4b-8590-ef70659b3436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e48c588-e385-4f9b-bd50-9958b9be6c0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"output.mp4\" controls  width=\"640\"  height=\"360\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ipd.display(ipd.Video(save_path, width=640, height=360))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef7ce67-7957-4e23-85dc-4281071e45d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
